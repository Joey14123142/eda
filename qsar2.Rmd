---
title: "QSAR Data Analysis"
output: html_document
---

## Objective

1. To develop a function that relates descriptors to toxicity.
2. To compare relative importance of QuickProp descriptors on toxicity prediction.

## Outline

In this experiment, regression models relating QuickProp descriptors to predict toxicity are built from a data set consists of 322 compounds that were experimentally assessed. 

## Require packages

```{r, warning=FALSE}
start <- function(pkg){
  npkg <- pkg[!(pkg %in% installed.packages()[,"Package"])]
  if (length(npkg))
    install.packages(npkg, dependencies = TRUE)
  lapply(pkg, require, character.only=TRUE)
}

pkgs <- c("QSARdata",'caret','ggplot2')
start(pkgs)
```



## Load data

```{r}
data(AquaticTox)
head(AquaticTox_Outcome)
descriptors<-AquaticTox_QuickProp
data<-cbind(descriptors,AquaticTox_Outcome$Activity)
str(data)
colnames(data)[51]<- "Activity"
colSums(is.na(data))/nrow(data)
```

The outcome is the negative log of activity labeled as "Activity". 

Missing values in each column can be ignored.



## Cleaning

Variables with correlations larger than 0.29 are omitted.

```{r}
data<-na.omit(data)

descs <- data[, !apply(data, 2, function(x) any(is.na(x)) )]

descs <- descs[, !apply( descs, 2, function(x) length(unique(x)) == 1 )]

r2 <- which(cor(descs[2:50])^2 > .29, arr.ind=TRUE)

r2 <- r2[ r2[,1] > r2[,2] , ]
d <- descs[, -unique(r2[,2])]
```



## Preprocessing

Box-Cox transformation was performed on each column for better normality. 

Dataset was split into training and test sets on a ratio of 8:2.

```{r}
Tran <- preProcess(d[,-1],method = "BoxCox")
data <-predict(Tran,d[,-1])

set.seed(909)
ind<-sample(2,nrow(data),replace=TRUE,prob=c(0.8,0.2))
training<-data[ind==1,-1]
training$ID <- seq.int(nrow(training))
test<-data[ind==2,-1]
```



## Linear least squares

Starting from a simple regression model.

```{r}
lm.fit <- lm(Activity ~ ., data = training[,-13])
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
```

Variables with high correlations are dropped in data cleaning section. So only 12 descriptors are used, among which the largest weight is -5.24, indicating an estimated 5.24 degrees decrease in activity for every unit increase of that variable holding the remaining variables constant.

### Significance testing

When testing an hypothesis with a categorical explanatory variable and a quantitative response variable, the tool normally used in statistics is Analysis of Variances, also called ANOVA.

```{r}
anova(lm.fit)
```

The first line tells us that adding a variable "QikProp_accptHB" is useful compared to a model with only an intercept because this single coefficient reduces the sum of squares by 35.404. 

The original sum of squares of the model with just an intercept is:
```{r}
mu1 <- mean(training$Activity)
initss<- sum((training$Activity-mu1)^2)
initss
```

In the ANOVA table, each row considers the reduction in the sum of squared residuals after adding coefficients compared to the model in the previous row.

The first column "DF" shows the "degrees of freedom" with each row. As the "QikProp_accptHB" variable introduced only one term to the model, DF value is 1.

The column "F value" indicates the mean of squares for the inclusion of the terms of interest (the sum of squares divided by the degrees of freedom) divided by the mean squared residuals (from the bottom row).

__Null Hypothesis__ The true value of the additional coefficient is 0.

__Alternative Hypothesis__ There is an effect from the chemiscal substructure on biological activity.

A p-values smaller than 0.05 (as suggested by normal scientific standard) indicates stronger evidence against the null hypothesis and thus we accept the alternative hypothesis.


### Residual plots:

```{r}
par(mfrow = c(2,2))
plot(lm.fit)
```

```{r}
plot(predict(lm.fit), residuals(lm.fit))

plot(predict(lm.fit), rstudent(lm.fit))
```

No systematic patterns or large outlying observations is detected from above residual plots.

Plot residuals versus molecules to zoom in the performance of the model:

```{r}
e <- resid(lm.fit)
n <- length(e)
x <- 1:n

plot(x, e,
     xlab = "Molecule index", 
     ylab = "Residuals", 
     bg = "lightblue", 
     col = "black", cex = 2, pch = 21,frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "blue" , lwd = 2)
```

Examining leverage values:

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

Molecules with indexes 170, 136 have much higher leverage values and produce larger residuals. We further check their dfbetas values:

```{r}
dfb <- data.frame(dfbetas(lm.fit))
summary(dfbetas(lm.fit)[-c(136,170),1])
```

With large leverage values and dfbetas, these two molecules are exerted.

Final model:

```{r}
newlm <- lm(Activity~., data = training[-c(170, 136),-13])
```

Visualising the performance of the final model:

```{r}
p1 <- data.frame(predict(newlm, test, interval = "confidence"))
p2 <- data.frame(predict(newlm, test, interval = "prediction"))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = 1:nrow(test)
p1$Activity <- test$Activity
p2$x = 1:nrow(test)
p2$Activity <- test$Activity
dat = rbind(p1, p2)
names(dat)[1] = "yhat"

ggplot(dat, aes(x, yhat)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2) +
  geom_line() +
  geom_point(aes(x, Activity), size = 4)
```
